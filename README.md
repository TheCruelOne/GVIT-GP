GVIT-GP: Injecting the Genomic Relationship Matrix as an Inductive Bias into a Vision Transformer via Cross-Attention for Genomic Prediction

Abstract
Applying Transformer architectures to genomic prediction (GP) is hindered by two fundamental challenges: the trade-off between computational cost and the integrity of genomic information, and the discrepancy between model complexity and the limited sample sizes typical of biological datasets. To address these challenges, we propose GViT-GP, a Vision Transformer (ViT) architecture specifically tailored for genotyping data. GViT-GP addresses the first challenge by employing a Selective Patch Embedding (SPE) strategy, which prioritizes and selects informative loci before tokenization, thereby preserving critical genetic information within computational constraints. Crucially, to resolve the conflict between model complexity and data scarcity, GViT-GP incorporates population structure, encapsulated in the Genomic Relationship Matrix (GRM), as a dynamic inductive bias. This is implemented through a dual-pathway, cross-attention architecture, wherein the SNP sequence serves as the query to actively integrate structural information from the GRM. This mechanism effectively mitigates the difficulty of training ViTs on small-sample datasets. Evaluated on benchmark datasets spanning 4 species and 20 traits, GViT-GP significantly outperforms four established methods, including GBLUP and LightGBM, across a majority of the predictive tasks. Our results demonstrate the effectiveness of GViT-GP and establish its potential as a promising next-generation framework for genomic prediction.

Code Status
Our manuscript is currently under peer review. To ensure the integrity of the review process, the full source code, pre-trained models, and detailed instructions will be made publicly available in this repository immediately upon official acceptance of the paper.
